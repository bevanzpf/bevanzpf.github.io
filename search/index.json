[{"content":"前阵子看了关于B站微服务化的视频分享，对其中讲到的服务发现相关的知识印象深刻，后面发现这个组件是开源的, 于是随手clone下来学习一下。discovery 是B站主站使用的服务注册/发现组件, 其设计参考服务注册发现领域的标杆Netflix Eureka并扩展了一些社区期望的功能，是一套纯粹的AP系统。Discovery实现了AP类型的服务注册发现，据说可用性极高，下面就来观摩一下源码，一探究竟。\n概况 \u0026hellip; TO BE CONTINUE \u0026hellip;\n","date":"2020-08-29","permalink":"https://nautilis.github.io/posts/learn-discovery-source-code/","tags":["Go","Microservices"],"title":"bilibili/discovery 源码解析"},{"content":"在大型业务系统中我们常常需要对数据库进行分库分表处理，此时数据的唯一标识无法再通过依赖数据库唯一键的方式实现，因此我们需要一个id服务负责分配唯一id。分布式id生成器在如今已经是再寻常不过的需求，业内有许多成熟的方案，最出名的有Twitter的snowflake算法，国内很多大厂也有自己的实现方案例如美团的Left,百度的uid-generator。今天我们通过Redis来实现一个分布式ID生成器，要求比较简单，除了id要唯一,支持分布式，还要保证定长跟无序。\n实现方法  多台机器ID保持唯一：id通过数字映射到35个数字字母组成(1-9,a-z, 0作为补位), 整个id分为两段，前段4位字符由当前时间跟服务的起始时间做差以及机器编号决定，后段5位字符则通过随机生成数字再通过Redis的bitmap排重。排重的key使用id的前段标识，redis bitmap可以最多可以标记2^32-1，后段位数为5共35^5=52521875个随机数,假如并发高到10000tps一个小时内也不会超过52521875了。 单个进程中ID的存取：通过channel存储生成的id, 当channel满时休眠，channel被消费有剩余空间时补充id。  package main import ( \u0026quot;bytes\u0026quot; \u0026quot;context\u0026quot; \u0026quot;github.com/go-redis/redis\u0026quot; \u0026quot;log\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) var redisClient *redis.Client type IdGenerate struct { base35code []byte startTime int64 machineCode int //服务器编码(通过hostname后缀获取） redisClient *redis.Client random *rand.Rand idChannel chan string //存储生成id的管道 } func NewIdGenerate() *IdGenerate { inst := \u0026amp;IdGenerate{ startTime: 1596782020, random: rand.New(rand.NewSource(time.Now().Unix())), idChannel: make(chan string, 10000), } hostName, _ := os.Hostname() machineId, _ := strconv.Atoi(hostName[len(hostName)-2:]) inst.machineCode = machineId base35Code := \u0026quot;123456789qwertyuioplkjhgfdsazxcvbnm\u0026quot; inst.base35code = []byte(base35Code) redisClient := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026quot;127.0.0.1:6379\u0026quot;, }) inst.redisClient = redisClient return inst } //将数字映射成字母数字组合字符串 func (this *IdGenerate) getBase35Code(num int64) string { codeLen := int64(len(this.base35code)) buf := bytes.Buffer{} for { c := this.base35code[num%codeLen] buf.WriteByte(c) num = num / codeLen if num \u0026lt;= 0 { break } } return buf.String() } //当前时间与服务起始时间做差来确定一个排重key func (this *IdGenerate) getDistinctKey() string { key := this.getBase35Code((time.Now().Unix()-this.startTime)/3600*100 + int64(this.machineCode)) //*100 预留两位给机器id pack := bytes.Buffer{} if len(key) \u0026lt; 4 { //不足四位通过0补齐 for i := len(key); i \u0026lt; 4; i++ { pack.WriteByte('0') } } pack.WriteString(key) return pack.String() } //通过bitmap过滤不断的尝试生成新id func (this *IdGenerate) genId() string { distinctKey := this.getDistinctKey() c, err := this.redisClient.BitCount(context.Background(), distinctKey, nil).Result() if err != nil { log.Print(err.Error()) } if c == 0 || c%100 == 0 { //设置新key 2小时后过期，这里怕一次设置可能失败故没在bitmap 每标记100个就重新设置一次过期 this.redisClient.Expire(context.Background(), distinctKey, 2*time.Hour) } var id int for { id = this.random.Intn(35) + 1 for i := 0; i \u0026lt; 4; i++ { r := this.random.Intn(35) + 1 id = id * r } val, err := this.redisClient.GetBit(context.Background(), distinctKey, int64(id)).Result() if err != nil { log.Print(\u0026quot;access redis error: \u0026quot; + err.Error()) } if val == 0 { _, err := this.redisClient.SetBit(context.Background(), distinctKey, int64(id), 1).Result() if err != nil { log.Println(err.Error()) continue } break } } randomPart := this.getBase35Code(int64(id)) b := bytes.Buffer{} b.WriteString(distinctKey) b.WriteString(randomPart) if len(randomPart) \u0026lt; 5 { for i := len(randomPart); i \u0026lt; 5; i++ { b.WriteByte('0') } } return b.String() } //循环不断生成id的方法 func (this *IdGenerate) provideId() { useOld := false var id string for { if !useOld { id = this.genId() } select { case this.idChannel \u0026lt;- id: log.Println(\u0026quot;add id:\u0026quot;, id) useOld = false default: useOld = true time.Sleep(2 * time.Second) } } } //获取id func (this *IdGenerate) ConsumeId(size int) (res []string) { //总的超时时间设置为100ms eachTimeOut := time.Duration(100/size) * time.Millisecond res = make([]string, size) for i := 0; i \u0026lt; size; i++ { select { case id := \u0026lt;-this.idChannel: res = append(res, id) case \u0026lt;-time.After(eachTimeOut): log.Println(\u0026quot;access id time\u0026quot;) } } return res } func main() { generator := NewIdGenerate() wg := sync.WaitGroup{} wg.Add(2) go func() { generator.provideId() wg.Done() }() for i := 0; i \u0026lt; 10; i++ { go func() { for { ids := generator.ConsumeId(3) log.Println(\u0026quot;consume ids:\u0026quot;, ids) time.Sleep(1 * time.Second) } }() } wg.Wait() }  ","date":"2020-08-15","permalink":"https://nautilis.github.io/posts/gen-unique-by-redis/","tags":["redis","Go"],"title":"徒手撸一个分布式id生成器"},{"content":"作为一个服务端开发人员，相信大家都有半夜被突然的告警惊醒的经历，也会有面临发生问题手忙脚乱无从下手的时期，这篇文章回顾了近期我排查一个线上性能问题的过程。\n背景 我主要负责的一个线上服务在夜深人静的某天调用方突然告服务超时,从上图可以看到高分期（23点到凌晨2点）响应时间严重的已经超过100ms。这是一个较为底层的服务，主要为用户feed流提供批量用户属性查询跟用户关系查询(获取feed流内容作者属性以及当前用户是否关注这些作者）。两种数据的获取通过内部调用另一个底层服务实现，对于用户属性，服务端先通过mget获取缓存中的数据，缓存不命中的情况下并发查询数据库再汇总返回；用户关系，也是先查询缓存再通过pipeline查询持久换缓存。机器部署了9台，其中1台4核，其他都是2核。\n初步排查  线上无新增代码，进程也是几天前的，排除新代码问题。 pv突然增长了经20%, 查看机器负载发现cpu,内存，load都很正常，依赖的Redis跟RDS也没有超水位告警。 真是见鬼，以往的排查方式居然没有一丝线索\u0026hellip; 于是我觉得自己加上耗时打点，然后通过脚本采集下耗时情况。  采集  接口加入打点统计不同机器上的mget耗时，redis未命中数， db并发查询耗时 在负载不高的机器分固定uid 跟随机uid两组请求接口获取耗时数据，同时本地直接连接redis调用mget统计耗时。  数据分析  计算每分钟各项数据耗时的平均值(这里走了个弯路😰) 对比不同机器上的mget耗时，看是不是个别机器的问题。 对比同时需要请求Redis跟DB时，了解mget跟db的耗时偏差。 对比服务mget跟本地mget耗时，验证Redis耗时情况是否如实。  以下图标x轴为时间（分钟级）y轴为耗时（单位ms) 上面这张就是不同机器的mget耗时情况，可以看出mget耗时确实存在高峰期效应并且不存在个别机器耗时偏离较大的情况，所以排除个别机器的问题。\n在同时需要查Redis跟DB的请求中，DB耗时明显高于Redis,但这是可预见的，高低峰两个曲线的波动从这个图上看不明显，进一步通过抽查服务的打点日志可以看到3万个请求中需要查询DB的只有16个，说明feed流获取的用户基本都是缓存到Redis的热数据，排除DB引发高耗时,进一步观察Redis的数据。\n上面这张图左边是出问题的服务内部mget，右边是低负载服务器上使用同个Redis集群的mget表现。两者呈现相同的分布趋势，Redis高峰期确实响应变慢。\n通过以上的一轮分析，首先排除了个别机器的问题以及DB的问题，似乎问题就是Redis慢了导致的。但是，不对啊，Redis耗时虽然高峰期变慢但是8ms也并不至于导致100+ms的告警，另外Redis的性能是大家公认的。 后面经过和同事的一番探讨，收获了一个重要的知识\u0026ndash;排查问题的时候一般看最大值，另外也还发现我漏分析了中转的耗时。\n上面这张就是后来补的中转耗时趋势图（按每分钟取最大值处理），中转最大耗时也不过2ms，显然也不是中转导致的。\n继续对之前的数据做最大值处理，在服务端mget跟本地mget这组队比中我们发现了异常👇\n在取最大值的情况下，服务的mget耗时高峰期普遍到达80ms以上，而直接连Redis的对照组则十分平稳没有高峰效应。这十分让人不解，我的脚本连接Redis跟服务器的表现完全对不上了，太奇怪了，是时候分析下代码了。\n查看代码  我的脚本跟服务器都是通过这样的方式获取Redis客户端，这个客户端到底是怎么生成的呢？option里到底有什么？   通过对代码的进一步查看，发现这个Redis客户端是维护了一组Redis链接池的，并且通过这个poolSize的参数设置链接池的大小，默认大小是10*CPU数。这下我们有了线索，初步怀疑连接不够用，继续看代码，发现连接池无空闲连接的时候默认的等待超时居然达1秒钟，这更加决定了我的怀疑，后续我加了个空闲连接数的带点也确实验证了高峰期连接数耗尽的情况。  解决  修改默认连接池数目，因为通过代码发现这个连接数是有自动回收机制的，连接空闲超过一定时间就会被回收。  总结  采用耗时打点的方式可以很快定位问题的位置。 设置合理的对照组有利于体现数据异常从中找到线索。 分析性能问题时要看最大值，平均值会掩盖掉真实情况。 最后要耐心地分析代码尝试到数据背后的真相。  ","date":"2020-07-31","permalink":"https://nautilis.github.io/posts/record-server-optimization/","tags":["redis"],"title":"记一次服务端性能优化"},{"content":"SSH是什么 ssh 是一种加密网络协议，通过建立安全隧道来进行客户端/服务器通信，通常用于远程登录，但实际上任何网络服务都可以通过ssh进行安全传输。\n利用SSH 进行科学上网  如果你恰巧有一台海外服务器，可以利用ssh进行动态端口转发从而实现科学上网。 ssh -p your_ssh_port -f -N -D 0.0.0.0:8000 username@remote_host 这个命令启动socket监听本地端口8000,将所有发送到的数据通过ssh协议与远程主机进行通信。 开启动态端口转发后，通过SwitchyOmega等代理插件将浏览器数据转发到本地端口8000即可。 定时重启脚本。不知道是不是超时配置问题，ssh会时不时断线，需要重启，所以就写了个python脚本加到crontab。  import subprocess import os import re ''' 通过 ps -ef | grep ssh\\ -p\\ 22\\ -f\\ -N | grep -v grep | awk '{print $2}' 找到启动的进程，执行 kill， 重启ssh转发命令。 ''' try: ps = subprocess.Popen(('ps', '-ef'), stdout=subprocess.PIPE) grep = subprocess.Popen(('grep', 'ssh\\ -p\\ 22\\ -f\\ -N' ), stdin=ps.stdout, stdout=subprocess.PIPE) awk = subprocess.check_output(('awk', '{print $2}'), stdin=grep.stdout) ps.wait() grep.wait() print(awk) except: command = \u0026quot;ssh -p 22 -f -N -D 0.0.0.0:8000 username@remote_ip\u0026quot; os.system(command) exit() pids = [p for p in awk.split('\\n') if re.match(\u0026quot;^\\d\u0026quot;, p)] for pid in pids: print(\u0026quot;to kill...%s\u0026quot; % (pid)) subprocess.check_output([\u0026quot;kill\u0026quot;, pid]) command = \u0026quot;ssh -p 22 -f -N -D 0.0.0.0:8000 username@remote_ip\u0026quot; os.system(command)   用shell 更少代码  #!/usr/bin pids=(`ps -ef | grep 'ssh -p 22 -f -N' | grep -v 'grep' | awk '{print $2}'`) echo $pids for p in $pids do kill -9 $p\tdone ssh -p 22 -f -N -D 0.0.0.0:1090 root@remote_ip  ","date":"2019-06-10","permalink":"https://nautilis.github.io/posts/access-google/","tags":["ssh"],"title":"科学上网之ssh"},{"content":"记录Java8的一些语法🍬\nList to Map @Data @AllArgsConstructor @ToString class People { private String name; private int age; private String lastName; } @Test public void testStream() { List\u0026lt;People\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(new People(\u0026quot;nautilis\u0026quot;, 25, \u0026quot;zheng\u0026quot;)); list.add(new People(\u0026quot;nautilis\u0026quot;, 25, \u0026quot;zheng\u0026quot;)); list.add(new People(\u0026quot;bevan\u0026quot;, 25, \u0026quot;zheng\u0026quot;)); list.add(new People(\u0026quot;ning\u0026quot;, 24, \u0026quot;li\u0026quot;)); //重复key只保留一个， 并用linkedHashMap保证顺序 Map\u0026lt;String, People\u0026gt; map = list.stream().sorted(Comparator.comparing(People::getAge).reversed()) .collect(Collectors.toMap(People::getName, Function.identity(), (oldv, newV) -\u0026gt; newV, LinkedHashMap::new )); System.out.printf(\u0026quot;重复key只保留一个， 并用linkedHashMap保证顺序---\u0026gt;\u0026quot;); map.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot;:\u0026quot; + v)); //姓氏#=\u0026gt;人列表 Map\u0026lt;String,List\u0026lt;People\u0026gt;\u0026gt; lastPeopleMap = list.stream() .collect(Collectors.groupingBy(People::getLastName)); System.out.println(\u0026quot;姓氏#=\u0026gt;人列表 ---\u0026gt;\u0026quot;); lastPeopleMap.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot; : \u0026quot; + v)); //姓氏#=\u0026gt; 人名列表 Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; lastnameNamelistMap = list.stream() .collect(Collectors.groupingBy(People::getLastName, Collectors.mapping(People::getName, Collectors.toList()))); System.out.println(\u0026quot;姓氏#=\u0026gt; 人名列表---\u0026gt;\u0026quot;); lastnameNamelistMap.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot; : \u0026quot; + v)); // 姓氏#=\u0026gt;人名集合 Map\u0026lt;String, Set\u0026lt;String\u0026gt;\u0026gt; lastnameNameSet = list.stream() .collect(Collectors.groupingBy(People::getLastName, Collectors.mapping(People::getName, Collectors.toSet()))); System.out.println(\u0026quot;姓氏#=\u0026gt;人名集合---\u0026gt;\u0026quot;); lastnameNameSet.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot; : \u0026quot; + v)); }  ","date":"2019-05-18","permalink":"https://nautilis.github.io/posts/java8/","tags":["Java"],"title":"Java8语法糖"},{"content":" 端口转发 http://www.dshowing.com/2017/08/14/SSH_portforward/ 本地启动ssh:ssh -p [serverport] -f -N -D 0.0.0.0:[socksport] username@remoteaddress chrome上用proxy switchysharp 或者 firefox 用FoxyProxy 配置一个socks5 IP:127.0.0.1 端口为所填的socksport。 ssh -o PubkeyAuthentication=no 120.24.161.131  不用ssh key ssh -f 后台运行 -N 不发送命令 -f -N 参数 ssh -f -N -L localport:host1:port1 host3 host2执行此命令， host2登录host3 本地端口转发 -L 来自host3走localhost的数据发送到host1的port1 远程端口转发 ssh -f -N -R remotePort:host1:port1 host3 在host2执行此命令表示 在host2登录host3 令 host3监听 remotePort, host2将host3发到 remotePort 的数据转发到host1:prot1 本地端口转发使用场景 host3可以连host2, host2能连host1,host3不能访问host1, 可通过host2做本地端口转发。 远程端口转发，用于内网穿透， host1 host2在内网，host2可访问外网的host3, host3无法访问内网，可以在host2做远程端口转发，使host3的数据发送到内网。 ssh -f -N -L localport:localhost:port host3 localhost指的是host3, localhost 是相对host3的。  ","date":"2019-03-21","permalink":"https://nautilis.github.io/posts/ssh-note/","tags":["ssh"],"title":"ssh 笔记"},{"content":"归并排序  归并排序是建立在归并操作的基础之上的。如果两个数组都是有序数组，只需迭代两个数组，不断比较两个数组元素，将较小的排到前面即可。归并排序首先通过递归对数组进行对等分割，直到分割的部分只有一个元素(相邻两个数组必定有序）进行合并操作。  #include\u0026lt;stdio.h\u0026gt; void show(int arr[], int n){ int i; for( i=0; i\u0026lt;n; i++) printf(\u0026quot;%d, \u0026quot;, arr[i]); printf(\u0026quot;\\n\u0026quot;); }\t//合并算法 void merge(int array[], int left, int mid, int right){ int aux[right+1]; //临时数组 //从中间断开两个标志分别 int i = left; int j = mid + 1; int k; for( k=left; k\u0026lt;=right; k++ ) //复制数组 aux[k] = array[k]; for( k=left; k\u0026lt;=right; k++ ){ if(i \u0026gt; mid) //左边指针已超出 array[k] = aux[j++]; else if(j \u0026gt; right) //右边指针超出 array[k] = aux[i++]; else if(aux[j] \u0026lt; aux[i]) //都没有超出就比较两者大小 array[k] = aux[j++]; else array[k] = aux[i++]; } } void sort(int array[], int left, int right){ if(left \u0026gt;= right) return;\tint mid = left + (right - left) / 2; sort(array, left, mid); sort(array, mid+1, right); merge(array, left, mid, right); } int main() { int arr_test[] = { 8, 2, 19, 4, 5, 11, 23, 44, 12, 31 }; int size = sizeof(arr_test)/sizeof(arr_test[0]); printf(\u0026quot;%s \\n\u0026quot;, \u0026quot;before sort:\u0026quot;); show(arr_test, size); sort(arr_test, 0, size-1);\tprintf(\u0026quot;%s \\n\u0026quot;, \u0026quot;after sorted:\u0026quot;); show(arr_test, size); return 0; }  快速排序  快速排序是建立在划分基础上的，划分过程选取一个数组成员作为比较将数组划分成大于这比较目标和小于这个比较目标的两部分,对数组不断的递归划分最终数组即是有序的。  #include\u0026lt;stdio.h\u0026gt; void show(int arr[], int size) { int i; for(i=0;i\u0026lt;size;i++) printf(\u0026quot;%d \u0026quot;, arr[i]); printf(\u0026quot;\\n\u0026quot;); } void exch(int arr[], int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } int partition(int arr[], int left, int right) { int v = arr[left]; int i = left; //向前扫的指针 int j = right + 1; //向后扫指针 while(1){ while(arr[++i] \u0026lt; v) if(i == right)//前扫指针已到低 break; while(arr[--j] \u0026gt; v) if(j == left) //后扫指针已到低 break; if(i\u0026gt;=j)// 两个指针相遇 break;\texch(arr, i, j); //while 循环停下来交换元素 } exch(arr, left, j); return j; } void sort(int arr[], int left, int right) { if(left \u0026gt;= right) return; int j = partition(arr, left, right); sort(arr, left, j); sort(arr, j+1, right); } void quickSort(int arr[], int size) { sort(arr, 0, size -1); } int main() { int arr[] = {23, 11, 24, 5, 67, 2, 4}; int size = sizeof(arr) / sizeof(arr[0]); show(arr, size); quickSort(arr, size); show(arr, size); }  ","date":"2018-07-07","permalink":"https://nautilis.github.io/posts/sort-algorithm/","tags":["sort"],"title":"排序算法"}]