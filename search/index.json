[{"content":"项目上大量使用Redis做缓存，前阵子还因为go-redis这个客户端的一些默认配置踩了些坑，所以决定抽出时间读读它的源码，好好了解一下一些关键功能（例如：连接池，redis命令）的实现。\n主要结构图 整个redis客户端分为两个主要的部分，一个是cmdable实现的各种Redis命令操作，另一部分是connPool实现的连接池管理功能。\n启动过程 func NewClient(opt *Options) *Client { opt.init() c := Client{ baseClient: newBaseClient(opt, newConnPool(opt)), ctx: context.Background(), } c.cmdable = c.Process return \u0026amp;c }  初始化配置 首先初始化配置对象options，这个struct主要包含Redis连接方式，各种读写超时以及连接池的相关属性设置，在opt.init中对各个属性进行设置，如果设置项没有指定则填充默认值。\ntype Options struct { // The network type, either tcp or unix. // Default is tcp. Network string // host:port address. Addr string // Dialer creates new network connection and has priority over // Network and Addr options. Dialer func(ctx context.Context, network, addr string) (net.Conn, error) // Hook that is called when new connection is established. OnConnect func(ctx context.Context, cn *Conn) error // Use the specified Username to authenticate the current connection // with one of the connections defined in the ACL list when connecting // to a Redis 6.0 instance, or greater, that is using the Redis ACL system. Username string // Optional password. Must match the password specified in the // requirepass server configuration option (if connecting to a Redis 5.0 instance, or lower), // or the User Password when connecting to a Redis 6.0 instance, or greater, // that is using the Redis ACL system. Password string // Database to be selected after connecting to the server. DB int // Maximum number of retries before giving up. // Default is to not retry failed commands. MaxRetries int // Minimum backoff between each retry. // Default is 8 milliseconds; -1 disables backoff. MinRetryBackoff time.Duration // Maximum backoff between each retry. // Default is 512 milliseconds; -1 disables backoff. MaxRetryBackoff time.Duration // Dial timeout for establishing new connections. // Default is 5 seconds. DialTimeout time.Duration // Timeout for socket reads. If reached, commands will fail // with a timeout instead of blocking. Use value -1 for no timeout and 0 for default. // Default is 3 seconds. ReadTimeout time.Duration // Timeout for socket writes. If reached, commands will fail // with a timeout instead of blocking. // Default is ReadTimeout. WriteTimeout time.Duration // Maximum number of socket connections. // Default is 10 connections per every CPU as reported by runtime.NumCPU. PoolSize int // Minimum number of idle connections which is useful when establishing // new connection is slow. MinIdleConns int // Connection age at which client retires (closes) the connection. // Default is to not close aged connections. MaxConnAge time.Duration // Amount of time client waits for connection if all connections // are busy before returning an error. // Default is ReadTimeout + 1 second. PoolTimeout time.Duration // Amount of time after which client closes idle connections. // Should be less than server's timeout. // Default is 5 minutes. -1 disables idle timeout check. IdleTimeout time.Duration // Frequency of idle checks made by idle connections reaper. // Default is 1 minute. -1 disables idle connections reaper, // but idle connections are still discarded by the client // if IdleTimeout is set. IdleCheckFrequency time.Duration // Enables read only queries on slave nodes. readOnly bool // TLS Config to use. When set TLS will be negotiated. TLSConfig *tls.Config // Limiter interface used to implemented circuit breaker or rate limiter. Limiter Limiter } func (opt *Options) init() { if opt.Addr == \u0026quot;\u0026quot; { opt.Addr = \u0026quot;localhost:6379\u0026quot; } if opt.Network == \u0026quot;\u0026quot; { if strings.HasPrefix(opt.Addr, \u0026quot;/\u0026quot;) { opt.Network = \u0026quot;unix\u0026quot; } else { opt.Network = \u0026quot;tcp\u0026quot; } } if opt.DialTimeout == 0 { opt.DialTimeout = 5 * time.Second } if opt.Dialer == nil { opt.Dialer = func(ctx context.Context, network, addr string) (net.Conn, error) { netDialer := \u0026amp;net.Dialer{ Timeout: opt.DialTimeout, KeepAlive: 5 * time.Minute, } if opt.TLSConfig == nil { return netDialer.DialContext(ctx, network, addr) } return tls.DialWithDialer(netDialer, network, addr, opt.TLSConfig) } } if opt.PoolSize == 0 { opt.PoolSize = 10 * runtime.NumCPU() } switch opt.ReadTimeout { case -1: opt.ReadTimeout = 0 case 0: opt.ReadTimeout = 3 * time.Second } switch opt.WriteTimeout { case -1: opt.WriteTimeout = 0 case 0: opt.WriteTimeout = opt.ReadTimeout } if opt.PoolTimeout == 0 { opt.PoolTimeout = opt.ReadTimeout + time.Second } if opt.IdleTimeout == 0 { opt.IdleTimeout = 5 * time.Minute } if opt.IdleCheckFrequency == 0 { opt.IdleCheckFrequency = time.Minute } if opt.MaxRetries == -1 { opt.MaxRetries = 0 } switch opt.MinRetryBackoff { case -1: opt.MinRetryBackoff = 0 case 0: opt.MinRetryBackoff = 8 * time.Millisecond } switch opt.MaxRetryBackoff { case -1: opt.MaxRetryBackoff = 0 case 0: opt.MaxRetryBackoff = 512 * time.Millisecond } }  创建连接池 连接池的配置承接自client的options，初始化连接池大小的配置后根据连接池的大小创建对应个数的连接填充到conns以及idlconns，这里通过一个带缓冲的queue channel来控制缓冲池的状态，省去通过数值比较每次都要加锁的损耗。初始化完成后通过启一个goruntine按指定的频率定时检测空闲连接中久未使用（超过idleTimeout）和超过最大存活时间的（猜测是由于连接建立超过一定时间可能因为某种原因已经断开TCP连接而不可用）。\nfunc NewConnPool(opt *Options) *ConnPool { p := \u0026amp;ConnPool{ opt: opt, queue: make(chan struct{}, opt.PoolSize), conns: make([]*Conn, 0, opt.PoolSize), idleConns: make([]*Conn, 0, opt.PoolSize), closedCh: make(chan struct{}), } p.connsMu.Lock() p.checkMinIdleConns() p.connsMu.Unlock() if opt.IdleTimeout \u0026gt; 0 \u0026amp;\u0026amp; opt.IdleCheckFrequency \u0026gt; 0 { go p.reaper(opt.IdleCheckFrequency) } return p }  指定执行命令的方法 c.cmdable = c.Process  代码跟踪到最终这个cmdable是位于baseClient#_process，各种Redis被封装成一个cmder然后再这个方法中通过获取连接，最后写入socket的方式执行Redis命令。\nfunc (c *baseClient) _process(ctx context.Context, cmd Cmder) error { var lastErr error for attempt := 0; attempt \u0026lt;= c.opt.MaxRetries; attempt++ { attempt := attempt var retry bool err := internal.WithSpan(ctx, \u0026quot;process\u0026quot;, func(ctx context.Context) error { if attempt \u0026gt; 0 { if err := internal.Sleep(ctx, c.retryBackoff(attempt)); err != nil { return err } } retryTimeout := true err := c.withConn(ctx, func(ctx context.Context, cn *pool.Conn) error { err := cn.WithWriter(ctx, c.opt.WriteTimeout, func(wr *proto.Writer) error { return writeCmd(wr, cmd) }) if err != nil { return err } err = cn.WithReader(ctx, c.cmdTimeout(cmd), cmd.readReply) if err != nil { retryTimeout = cmd.readTimeout() == nil return err } return nil }) if err == nil { return nil } retry = shouldRetry(err, retryTimeout) return err }) if err == nil || !retry { return err } lastErr = err } return lastErr }  连接池的实现 如何获取连接 获取连接时通过queue这个缓冲信号通道来判断连接池是否已经使用满，在p.waitTurn这个方法中首先尝试向queue发送信号，如何queue阻塞说明缓冲池已经用满，需要在一定的timeout时间内不断重试。这里的超时timers采用了池化，避免连接池持续满负荷时需要持续创建timer小对象导致不断GC。\nvar timers = sync.Pool{ New: func() interface{} { t := time.NewTimer(time.Hour) t.Stop() return t }, } func (p *ConnPool) waitTurn(ctx context.Context) error { select { case \u0026lt;-ctx.Done(): return ctx.Err() default: } select { case p.queue \u0026lt;- struct{}{}: return nil default: } timer := timers.Get().(*time.Timer) timer.Reset(p.opt.PoolTimeout) select { case \u0026lt;-ctx.Done(): if !timer.Stop() { \u0026lt;-timer.C } timers.Put(timer) return ctx.Err() case p.queue \u0026lt;- struct{}{}: if !timer.Stop() { \u0026lt;-timer.C } timers.Put(timer) return nil case \u0026lt;-timer.C: timers.Put(timer) atomic.AddUint32(\u0026amp;p.stats.Timeouts, 1) return ErrPoolTimeout } } // Get returns existed connection from the pool or creates a new one. func (p *ConnPool) Get(ctx context.Context) (*Conn, error) { if p.closed() { return nil, ErrClosed } err := p.waitTurn(ctx) if err != nil { return nil, err } for { p.connsMu.Lock() cn := p.popIdle() p.connsMu.Unlock() if cn == nil { break } if p.isStaleConn(cn) { _ = p.CloseConn(cn) continue } atomic.AddUint32(\u0026amp;p.stats.Hits, 1) return cn, nil } atomic.AddUint32(\u0026amp;p.stats.Misses, 1) newcn, err := p.newConn(ctx, true) if err != nil { p.freeTurn() return nil, err } return newcn, nil }  如何放回连接池 连接被使用完毕后调用pool.Put返回连接池，Put方法中首先检查连接是否还有没有被读取的数据，如果有则关闭连接放回BadConnError。判断是否为连接池中的连接，如果不是则直接关闭。最后将连接放回idleConns队列，从queue通道移除一个元素。\nfunc (p *ConnPool) Put(cn *Conn) { if cn.rd.Buffered() \u0026gt; 0 { internal.Logger.Printf(context.Background(), \u0026quot;Conn has unread data\u0026quot;) p.Remove(cn, BadConnError{}) return } if !cn.pooled { p.Remove(cn, nil) return } p.connsMu.Lock() p.idleConns = append(p.idleConns, cn) p.idleConnsLen++ p.connsMu.Unlock() p.freeTurn() }  如何建立连接 建立连接的过程其实就是调用options中连接Redis的网络方法，返回connection再封装成*Conn, 当连接创建失败的次数到达连接池的size时触发不断重连直到成功\nfunc (p *ConnPool) newConn(ctx context.Context, pooled bool) (*Conn, error) { cn, err := p.dialConn(ctx, pooled) if err != nil { return nil, err } p.connsMu.Lock() p.conns = append(p.conns, cn) if pooled { // If pool is full remove the cn on next Put. if p.poolSize \u0026gt;= p.opt.PoolSize { cn.pooled = false } else { p.poolSize++ } } p.connsMu.Unlock() return cn, nil } func (p *ConnPool) dialConn(ctx context.Context, pooled bool) (*Conn, error) { if p.closed() { return nil, ErrClosed } if atomic.LoadUint32(\u0026amp;p.dialErrorsNum) \u0026gt;= uint32(p.opt.PoolSize) { return nil, p.getLastDialError() } netConn, err := p.opt.Dialer(ctx) if err != nil { p.setLastDialError(err) if atomic.AddUint32(\u0026amp;p.dialErrorsNum, 1) == uint32(p.opt.PoolSize) { go p.tryDial() } return nil, err } internal.NewConnectionsCounter.Add(ctx, 1) cn := NewConn(netConn) cn.pooled = pooled return cn, nil } func (p *ConnPool) tryDial() { for { if p.closed() { return } conn, err := p.opt.Dialer(context.Background()) if err != nil { p.setLastDialError(err) time.Sleep(time.Second) continue } atomic.StoreUint32(\u0026amp;p.dialErrorsNum, 0) _ = conn.Close() return } }  ","date":"2020-11-19","permalink":"/posts/go-redis-source-code-learning/","tags":["Go","redis"],"title":"go-redis/redis 源码学习"},{"content":"go generate 是 go 1.4版本新增的一个特性，用于在编译前通过命令自动生成代码，例如官方提供的stringer（golang.org/x/tools/cmd/stringer）。该注释只在.go源文件中生效，并且需要显示调用go generate命令才会执行。在实际项目中经常需要定义数据表实体来对应数据库中的某一张表，每次定义实体都需要重复比对数据库表字段费时费力还容易错漏，所以萌生用go generate这个特性解决这个重复工作的想法。\n预期效果  定义一个空实体 标注 //go:generate entity-helper -target StructName -table TableName 就可以自动填充对应字段及注释并且格式化  //go:generate entity-helper -target User -table user type User struct { }  实现方式  通过go标准库的go/ast go/parse 解析package的语法树结构，定位到目标结构体 查询数据库表结构映射生成golang代码  语法树的类型比较多 最好先通过ast.Print(fset, f)的方式将每个文件的结构打印出来在慢慢定位\npkgInfo, err = build.ImportDir(\u0026quot;./\u0026quot;, 0) if err != nil { return 0, 0, \u0026quot;\u0026quot;, errors.Wrap(err, \u0026quot;fail to import Dir\u0026quot;) } fset := token.NewFileSet() for _, file := range pkgInfo.GoFiles { f, err := parser.ParseFile(fset, file, nil, 0) if err != nil { log.Fatal(err) } //ast.Print(fset, f) ast.Inspect(f, func(node ast.Node) bool { decl, ok := node.(*ast.GenDecl) if !ok { return true } // 一层层解析语法树 } }  完整代码  https://github.com/nautilis/goentity-helper  ","date":"2020-11-13","permalink":"/posts/golang-generate-entity/","tags":["Go"],"title":"go generate 实战☞数据表实体生成器"},{"content":"前阵子看了关于B站微服务化的视频分享，对其中讲到的服务发现相关的知识印象深刻，后面发现这个组件是开源的, 于是随手clone下来学习一下。discovery 是B站主站使用的服务注册/发现组件, 其设计参考服务注册发现领域的标杆Netflix Eureka并扩展了一些社区期望的功能，是一套纯粹的AP系统。Discovery实现了AP类型的服务注册发现，据说可用性极高，下面就来观摩一下源码，一探究竟。\n概况 整个名字服务是一个依赖于discovery的HTTP服务器，对外提供了服务注册，服务信息拉取，反注册，心跳等方法。discovery关联了一个register的结构，register包括了几个部分的数据，注册到discovery的实例Apps，调度相关配置scheduler，服务健康检查与自我保护用到的相关指标数据Guard，当前长轮询查询连接的信息hosts，以及discovery集群的节点数据Nodes。\n启动过程 func main() { flag.Parse() if err := conf.Init(); err != nil { log.Error(\u0026quot;conf.Init() error(%v)\u0026quot;, err) panic(err) } log.Init(conf.Conf.Log) dis, cancel := discovery.New(conf.Conf) http.Init(conf.Conf, dis) // init signal c := make(chan os.Signal, 1) signal.Notify(c, syscall.SIGHUP, syscall.SIGQUIT, syscall.SIGTERM, syscall.SIGINT) for { s := \u0026lt;-c log.Info(\u0026quot;discovery get a signal %s\u0026quot;, s.String()) switch s { case syscall.SIGQUIT, syscall.SIGTERM, syscall.SIGINT: cancel() time.Sleep(time.Second) log.Info(\u0026quot;discovery quit !!!\u0026quot;) return case syscall.SIGHUP: default: return } } }  从入口函数开始，首先用paladin这个bilibili自研的配置加载器解析配置，然后创建discovery实例，之后根据discovery实例创建一个HTTP服务对外提供服务，最后就是常见的监听系统信号在服务退出的时候做一些自我反注册的扫尾工作。我们具体关注下discovery实例的初始化过程。\ndiscovery实例初始化 func New(c *conf.Config) (d *Discovery, cancel context.CancelFunc) { d = \u0026amp;Discovery{ protected: c.EnableProtect, c: c, client: http.NewClient(c.HTTPClient), registry: registry.NewRegistry(c), } d.nodes.Store(registry.NewNodes(c)) //存储 discovery节点信息 d.syncUp() //拉取其他节点上的所有注册信息, 注册到当前节点 广播 cancel = d.regSelf() // 本节点信息注册以及心跳, cancel被调用后会去掉注册 go d.nodesproc() // 不断更新节点信息 go d.exitProtect() // 启动60s后退出保护模式 return }   首先创建了一个http客户端，这个主要是后续用来与其他节点通讯 创建Registry 从配置加载其他节点信息 同步其他节点上注册的app的信息 监听节点变更信息 自我注册 最后在60s后（确保discovery服务以及启动并且稳定运行）退出保护模式  创建Registry // NewRegistry new register. func NewRegistry(conf *conf.Config) (r *Registry) { r = \u0026amp;Registry{ appm: make(map[string]*model.Apps), conns: make(map[string]*hosts), gd: new(Guard), } r.scheduler = newScheduler(r) r.scheduler.Load() go r.scheduler.Reload() go r.proc() return }   appm用于存储注册上来的服务信息，appid-env =\u0026gt; Apps, Apps是根据zone分组的 zone =\u0026gt; App, App 存了一个根据hostname 分组的instances字典。（appid-env =\u0026gt; zone =\u0026gt; hostname =\u0026gt; instance） conns存储当前连接到这个节点的请求数据，conns是根据env跟appids分组的hosts字典（env.appid =\u0026gt; hosts），hosts是根据hostname分组的conn字典。每个conn存储了请求的参数信息，以及一个channel，当注册实例信息发生变时通过通过这个channel对长轮询请求发送信号返回最新注册实例数据。 scheduler是服务调度相关的配置，应该是设置每个服务的权重信息之类的，没细看。 gd存储的是一些服务心跳统计相关的数据，在下面的proc()方法中会每隔一分钟就统计更新一次前一分钟收到的心跳数，并且开启分批逐次剔除一些不可用的服务（非保护模式下90s没有心跳的服务），值得注意的是不管符合剔除条件的服务数据有多少，每次最多只能随机剔除服务总数的25%的不可用服务。  func (r *Registry) evict() { protect := r.gd.ok() // We collect first all expired items, to evict them in random order. For large eviction sets, // if we do not that, we might wipe out whole apps before self preservation kicks in. By randomizing it, // the impact should be evenly distributed across all applications. var eis []*model.Instance var registrySize int // all projects ass := r.allapp() for _, as := range ass { for _, a := range as.App(\u0026quot;\u0026quot;) { registrySize += a.Len() is := a.Instances() for _, i := range is { delta := time.Now().UnixNano() - i.RenewTimestamp if (!protect \u0026amp;\u0026amp; delta \u0026gt; _evictThreshold) || delta \u0026gt; _evictCeiling { eis = append(eis, i) } } } } // To compensate for GC pauses or drifting local time, we need to use current registry size as a base for // triggering self-preservation. Without that we would wipe out full registry. eCnt := len(eis) registrySizeThreshold := int(float64(registrySize) * _percentThreshold) evictionLimit := registrySize - registrySizeThreshold if eCnt \u0026gt; evictionLimit { eCnt = evictionLimit } if eCnt == 0 { return } for i := 0; i \u0026lt; eCnt; i++ { // Pick a random item (Knuth shuffle algorithm) next := i + rand.Intn(len(eis)-i) eis[i], eis[next] = eis[next], eis[i] ei := eis[i] r.cancel(ei.Zone, ei.Env, ei.AppID, ei.Hostname, time.Now().UnixNano()) } }  每隔15分钟更新一次期望心跳数，期望心跳数为注册到当前节点的实例总数*2 * 0.82，当每分钟接收到的心跳总数小于这个期望值的时候服务进入保护模式不进行随机剔除不可用节点的操作。\n加载其他节点信息 discovery支持集群部署，通过配置加载其他节点的信息以及其他数据中心的节点的信息，用于后续于其他节点通讯， Nodes最终通过atom.Value存起来\n// NewNodes new nodes and return. func NewNodes(c *conf.Config) *Nodes { nodes := make([]*Node, 0, len(c.Nodes)) for _, addr := range c.Nodes { // 当前数据中心的节点信息 n := newNode(c, addr) n.zone = c.Env.Zone n.pRegisterURL = fmt.Sprintf(\u0026quot;http://%s%s\u0026quot;, c.HTTPServer.Addr, _registerURL) // 注册的http路径 nodes = append(nodes, n) } zones := make(map[string][]*Node) for name, addrs := range c.Zones { // 其他数据中心的节点信息 var znodes []*Node for _, addr := range addrs { n := newNode(c, addr) n.otherZone = true n.zone = name n.pRegisterURL = fmt.Sprintf(\u0026quot;http://%s%s\u0026quot;, c.HTTPServer.Addr, _registerURL) znodes = append(znodes, n) } zones[name] = znodes } return \u0026amp;Nodes{ nodes: nodes, zones: zones, selfAddr: c.HTTPServer.Addr, // 当前节点地址 }  syncUp同步其他节点的注册实例 便利其他节点，逐个全量拉取注册在这些节点上的服务，然后将这些服务的注册信息注册到当前节点上，也就是存储到registry的appm结构中，同时对关注这个appid的长轮询进行通知（这个注册\u0026ndash;长轮询拉取的交互过程后续在做具体解析）\nregSelf自注册 将本节点作为实例服务注册到discovery集群，注册到本节点 同时注册到所有其他集群中的节点。\nfunc (d *Discovery) Register(c context.Context, ins *model.Instance, latestTimestamp int64, replication bool, fromzone bool) { _ = d.registry.Register(ins, latestTimestamp) if !replication { //下面的方法对所有nodes都注册本节点 _ = d.nodes.Load().(*registry.Nodes).Replicate(c, model.Register, ins, fromzone) } }  注册完后没30s对集群中的所有节点发送心跳，检测到取消信号时进行反注册\ngo func() { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for { select { case \u0026lt;-ticker.C: arg := \u0026amp;model.ArgRenew{ AppID: ins.AppID, Zone: d.c.Env.Zone, Env: d.c.Env.DeployEnv, Hostname: d.c.Env.Host, } if _, err := d.Renew(ctx, arg); err != nil \u0026amp;\u0026amp; err == ecode.NothingFound { d.Register(ctx, ins, now, false, false) } case \u0026lt;-ctx.Done(): arg := \u0026amp;model.ArgCancel{ AppID: model.AppID, Zone: d.c.Env.Zone, Env: d.c.Env.DeployEnv, Hostname: d.c.Env.Host, } if err := d.Cancel(context.Background(), arg); err != nil { log.Error(\u0026quot;d.Cancel(%+v) error(%v)\u0026quot;, arg, err) } return } } }()  nodesproc监听节点变化 如上面的流程，集群中新增某个节点这个节点就会将自己注册到这个集群中去，所以节点启动完成后除了配置文件中的节点，集群中还可能动态新增了节点，nodesproc就是监听节点变化的方法通过polls监听appid=infra.discovery的服务变更，每次检测到变更则更新Nodes这个atom.Value\nfunc (d *Discovery) nodesproc() { var ( lastTs int64 ) for { arg := \u0026amp;model.ArgPolls{ AppID: []string{model.AppID}, Env: d.c.Env.DeployEnv, Hostname: d.c.Env.Host, LatestTimestamp: []int64{lastTs}, } ch, _, _, err := d.registry.Polls(arg) if err != nil \u0026amp;\u0026amp; err != ecode.NotModified { log.Error(\u0026quot;d.registry(%v) error(%v)\u0026quot;, arg, err) time.Sleep(time.Second) continue } apps := \u0026lt;-ch ins, ok := apps[model.AppID] if !ok || ins == nil { return } var ( nodes []string zones = make(map[string][]string) ) for _, ins := range ins.Instances { for _, in := range ins { for _, addr := range in.Addrs { u, err := url.Parse(addr) if err == nil \u0026amp;\u0026amp; u.Scheme == \u0026quot;http\u0026quot; { if in.Zone == d.c.Env.Zone { nodes = append(nodes, u.Host) } else { zones[in.Zone] = append(zones[in.Zone], u.Host) } } } } } lastTs = ins.LatestTimestamp c := new(conf.Config) *c = *d.c c.Nodes = nodes c.Zones = zones ns := registry.NewNodes(c) ns.UP() d.nodes.Store(ns) log.Info(\u0026quot;discovery changed nodes:%v zones:%v\u0026quot;, nodes, zones) } }  注册 每个需要注册的服务通过调discovery的/register接口将自身注册上来，注册参数如下。\ntype ArgRegister struct { Region string `form:\u0026quot;region\u0026quot;` Zone string `form:\u0026quot;zone\u0026quot; validate:\u0026quot;required\u0026quot;` Env string `form:\u0026quot;env\u0026quot; validate:\u0026quot;required\u0026quot;` AppID string `form:\u0026quot;appid\u0026quot; validate:\u0026quot;required\u0026quot;` Hostname string `form:\u0026quot;hostname\u0026quot; validate:\u0026quot;required\u0026quot;` Status uint32 `form:\u0026quot;status\u0026quot; validate:\u0026quot;required\u0026quot;` Addrs []string `form:\u0026quot;addrs\u0026quot; validate:\u0026quot;gt=0\u0026quot;` Version string `form:\u0026quot;version\u0026quot;` Metadata string `form:\u0026quot;metadata\u0026quot;` Replication bool `form:\u0026quot;replication\u0026quot;` // 是否属于discovery节点间复制的请求 LatestTimestamp int64 `form:\u0026quot;latest_timestamp\u0026quot;` DirtyTimestamp int64 `form:\u0026quot;dirty_timestamp\u0026quot;` FromZone bool `form:\u0026quot;from_zone\u0026quot;` }  解析完这些参数组装成一个instance实例最后存储到registry#appm，同时向discovery集群的其他节点注册这个instance\n// Register a new instance. func (d *Discovery) Register(c context.Context, ins *model.Instance, latestTimestamp int64, replication bool, fromzone bool) { _ = d.registry.Register(ins, latestTimestamp) if !replication { // Replicate 会同步集群的其他节点 _ = d.nodes.Load().(*registry.Nodes).Replicate(c, model.Register, ins, fromzone) } }  每当有新实例加入进来，会更新Registry的心跳期望 r.gd.incrExp() , 同时对正在进行长轮询订阅这个实例的连接进行广播。\n// Register a new instance. func (r *Registry) Register(ins *model.Instance, latestTime int64) (err error) { a := r.newApp(ins) i, ok := a.NewInstance(ins, latestTime) if ok { r.gd.incrExp() // 增加心跳期望值 } // NOTE: make sure free poll before update appid latest timestamp. r.broadcast(i.Env, i.AppID) // 对长轮询进行广播返回结果 return }  广播就是Fetch这个appid的所有实例信息出来，根据每个长轮询的条件进行过滤，最后通过channel发送过滤出来的结果集，长轮询那边接收到channel信号即返回结果。\n// broadcast on poll by chan. // NOTE: make sure free poll before update appid latest timestamp. func (r *Registry) broadcast(env, appid string) { key := pollKey(env, appid) r.cLock.Lock() conns, ok := r.conns[key] if !ok { r.cLock.Unlock() return } delete(r.conns, key) r.cLock.Unlock() conns.hclock.RLock() for _, conn := range conns.hosts { ii, err := r.Fetch(conn.arg.Zone, env, appid, 0, model.InstanceStatusUP) // TODO(felix): latesttime!=0 increase if err != nil { // may be not found ,just continue until next poll return err. log.Error(\u0026quot;get appid:%s env:%s zone:%s err:%v\u0026quot;, appid, env, conn.arg.Zone, err) continue } for i := 0; i \u0026lt; conn.count; i++ { select { case conn.ch \u0026lt;- map[string]*model.InstanceInfo{appid: ii}: // NOTE: if chan is full, means no poller. log.Info(\u0026quot;broadcast to(%s) success(%d)\u0026quot;, conn.arg.Hostname, i+1) case \u0026lt;-time.After(time.Millisecond * 500): log.Info(\u0026quot;broadcast to(%s) failed(%d) maybe chan full\u0026quot;, conn.arg.Hostname, i+1) } } } conns.hclock.RUnlock() }  拉取 拉取通过fetch过滤取argPolls中指定的app实例，Fetch方法通过对比appm中存储的实例的latestTimestamp, 如果latestTimestamp \u0026lt; 存储的实例的lastTimestamp 说明注册中的实例变更还没有被拉取到，直接返回这个实例的信息。如果latestTimestamp \u0026gt;= 存储实例的lastTimestamp, 说明之前拉取到的已经是最新的实例注册信息，这个时候回阻塞等待channel信号。\n// Polls hangs request and then write instances when that has changes, or return NotModified. func (r *Registry) Polls(arg *model.ArgPolls) (ch chan map[string]*model.InstanceInfo, new bool, miss []string, err error) { var ( ins = make(map[string]*model.InstanceInfo, len(arg.AppID)) ) if len(arg.AppID) != len(arg.LatestTimestamp) { arg.LatestTimestamp = make([]int64, len(arg.AppID)) } for i := range arg.AppID { in, err := r.Fetch(arg.Zone, arg.Env, arg.AppID[i], arg.LatestTimestamp[i], model.InstanceStatusUP) if err == ecode.NothingFound { miss = append(miss, arg.AppID[i]) log.Error(\u0026quot;Polls zone(%s) env(%s) appid(%s) error(%v)\u0026quot;, arg.Zone, arg.Env, arg.AppID[i], err) continue } if err == nil { ins[arg.AppID[i]] = in new = true } } if new { ch = make(chan map[string]*model.InstanceInfo, 1) ch \u0026lt;- ins return } for i := range arg.AppID { k := pollKey(arg.Env, arg.AppID[i]) r.cLock.Lock() if _, ok := r.conns[k]; !ok { r.conns[k] = \u0026amp;hosts{hosts: make(map[string]*conn, 1)} } hosts := r.conns[k] r.cLock.Unlock() hosts.hclock.Lock() connection, ok := hosts.hosts[arg.Hostname] if !ok { if ch == nil { ch = make(chan map[string]*model.InstanceInfo, 5) // NOTE: there maybe have more than one connection on the same hostname!!! } connection = newConn(ch, arg.LatestTimestamp[i], arg) log.Info(\u0026quot;Polls from(%s) new connection(%d)\u0026quot;, arg.Hostname, connection.count) } else { connection.count++ // NOTE: there maybe have more than one connection on the same hostname!!! if ch == nil { ch = connection.ch } log.Info(\u0026quot;Polls from(%s) reuse connection(%d)\u0026quot;, arg.Hostname, connection.count) } hosts.hosts[arg.Hostname] = connection hosts.hclock.Unlock() } return }  心跳 每个注册到discovery的服务需要每隔30s就向discovery发送一次心跳，discovery更新app的renew时间戳，同时更新收到当前分钟收到的心跳数 atomic.AddInt64(\u0026amp;g.facInMin, 1) 。同时每个节点接收到的心跳也会复制给其他节点。\n// Renew marks the given instance of the given app name as renewed, and also marks whether it originated from replication. func (d *Discovery) Renew(c context.Context, arg *model.ArgRenew) (i *model.Instance, err error) { i, ok := d.registry.Renew(arg) if !ok { err = ecode.NothingFound log.Error(\u0026quot;renew appid(%s) hostname(%s) zone(%s) env(%s) error\u0026quot;, arg.AppID, arg.Hostname, arg.Zone, arg.Env) return } if !arg.Replication { _ = d.nodes.Load().(*registry.Nodes).Replicate(c, model.Renew, i, arg.Zone != d.c.Env.Zone) return } if arg.DirtyTimestamp \u0026gt; i.DirtyTimestamp { err = ecode.NothingFound } else if arg.DirtyTimestamp \u0026lt; i.DirtyTimestamp { err = ecode.Conflict } return }  ","date":"2020-08-29","permalink":"/posts/learn-discovery-source-code/","tags":["Go","Microservices"],"title":"bilibili/discovery 源码解析"},{"content":"在大型业务系统中我们常常需要对数据库进行分库分表处理，此时数据的唯一标识无法再通过依赖数据库唯一键的方式实现，因此我们需要一个id服务负责分配唯一id。分布式id生成器在如今已经是再寻常不过的需求，业内有许多成熟的方案，最出名的有Twitter的snowflake算法，国内很多大厂也有自己的实现方案例如美团的Left,百度的uid-generator。今天我们通过Redis来实现一个分布式ID生成器，要求比较简单，除了id要唯一,支持分布式，还要保证定长跟无序。\n实现方法  多台机器ID保持唯一：id通过数字映射到35个数字字母组成(1-9,a-z, 0作为补位), 整个id分为两段，前段4位字符由当前时间跟服务的起始时间做差以及机器编号决定，后段5位字符则通过随机生成数字再通过Redis的bitmap排重。排重的key使用id的前段标识，redis bitmap可以最多可以标记2^32-1，后段位数为5共35^5=52521875个随机数,假如并发高到10000tps一个小时内也不会超过52521875了。 单个进程中ID的存取：通过channel存储生成的id, 当channel满时休眠，channel被消费有剩余空间时补充id。  package main import ( \u0026quot;bytes\u0026quot; \u0026quot;context\u0026quot; \u0026quot;github.com/go-redis/redis\u0026quot; \u0026quot;log\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) var redisClient *redis.Client type IdGenerate struct { base35code []byte startTime int64 machineCode int //服务器编码(通过hostname后缀获取） redisClient *redis.Client random *rand.Rand idChannel chan string //存储生成id的管道 } func NewIdGenerate() *IdGenerate { inst := \u0026amp;IdGenerate{ startTime: 1596782020, random: rand.New(rand.NewSource(time.Now().Unix())), idChannel: make(chan string, 10000), } hostName, _ := os.Hostname() machineId, _ := strconv.Atoi(hostName[len(hostName)-2:]) inst.machineCode = machineId base35Code := \u0026quot;123456789qwertyuioplkjhgfdsazxcvbnm\u0026quot; inst.base35code = []byte(base35Code) redisClient := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026quot;127.0.0.1:6379\u0026quot;, }) inst.redisClient = redisClient return inst } //将数字映射成字母数字组合字符串 func (this *IdGenerate) getBase35Code(num int64) string { codeLen := int64(len(this.base35code)) buf := bytes.Buffer{} for { c := this.base35code[num%codeLen] buf.WriteByte(c) num = num / codeLen if num \u0026lt;= 0 { break } } return buf.String() } //当前时间与服务起始时间做差来确定一个排重key func (this *IdGenerate) getDistinctKey() string { key := this.getBase35Code((time.Now().Unix()-this.startTime)/3600*100 + int64(this.machineCode)) //*100 预留两位给机器id pack := bytes.Buffer{} if len(key) \u0026lt; 4 { //不足四位通过0补齐 for i := len(key); i \u0026lt; 4; i++ { pack.WriteByte('0') } } pack.WriteString(key) return pack.String() } //通过bitmap过滤不断的尝试生成新id func (this *IdGenerate) genId() string { distinctKey := this.getDistinctKey() c, err := this.redisClient.BitCount(context.Background(), distinctKey, nil).Result() if err != nil { log.Print(err.Error()) } if c == 0 || c%100 == 0 { //设置新key 2小时后过期，这里怕一次设置可能失败故没在bitmap 每标记100个就重新设置一次过期 this.redisClient.Expire(context.Background(), distinctKey, 2*time.Hour) } var id int for { id = this.random.Intn(35) + 1 for i := 0; i \u0026lt; 4; i++ { r := this.random.Intn(35) + 1 id = id * r } val, err := this.redisClient.GetBit(context.Background(), distinctKey, int64(id)).Result() if err != nil { log.Print(\u0026quot;access redis error: \u0026quot; + err.Error()) } if val == 0 { _, err := this.redisClient.SetBit(context.Background(), distinctKey, int64(id), 1).Result() if err != nil { log.Println(err.Error()) continue } break } } randomPart := this.getBase35Code(int64(id)) b := bytes.Buffer{} b.WriteString(distinctKey) b.WriteString(randomPart) if len(randomPart) \u0026lt; 5 { for i := len(randomPart); i \u0026lt; 5; i++ { b.WriteByte('0') } } return b.String() } //循环不断生成id的方法 func (this *IdGenerate) provideId() { useOld := false var id string for { if !useOld { id = this.genId() } select { case this.idChannel \u0026lt;- id: log.Println(\u0026quot;add id:\u0026quot;, id) useOld = false default: useOld = true time.Sleep(2 * time.Second) } } } //获取id func (this *IdGenerate) ConsumeId(size int) (res []string) { //总的超时时间设置为100ms eachTimeOut := time.Duration(100/size) * time.Millisecond res = make([]string, size) for i := 0; i \u0026lt; size; i++ { select { case id := \u0026lt;-this.idChannel: res = append(res, id) case \u0026lt;-time.After(eachTimeOut): log.Println(\u0026quot;access id time\u0026quot;) } } return res } func main() { generator := NewIdGenerate() wg := sync.WaitGroup{} wg.Add(2) go func() { generator.provideId() wg.Done() }() for i := 0; i \u0026lt; 10; i++ { go func() { for { ids := generator.ConsumeId(3) log.Println(\u0026quot;consume ids:\u0026quot;, ids) time.Sleep(1 * time.Second) } }() } wg.Wait() }  ","date":"2020-08-15","permalink":"/posts/gen-unique-by-redis/","tags":["redis","Go"],"title":"徒手撸一个分布式id生成器"},{"content":"作为一个服务端开发人员，相信大家都有半夜被突然的告警惊醒的经历，也会有面临发生问题手忙脚乱无从下手的时期，这篇文章回顾了近期我排查一个线上性能问题的过程。\n背景 我主要负责的一个线上服务在夜深人静的某天调用方突然告服务超时,从上图可以看到高分期（23点到凌晨2点）响应时间严重的已经超过100ms。这是一个较为底层的服务，主要为用户feed流提供批量用户属性查询跟用户关系查询(获取feed流内容作者属性以及当前用户是否关注这些作者）。两种数据的获取通过内部调用另一个底层服务实现，对于用户属性，服务端先通过mget获取缓存中的数据，缓存不命中的情况下并发查询数据库再汇总返回；用户关系，也是先查询缓存再通过pipeline查询持久换缓存。机器部署了9台，其中1台4核，其他都是2核。\n初步排查  线上无新增代码，进程也是几天前的，排除新代码问题。 pv突然增长了经20%, 查看机器负载发现cpu,内存，load都很正常，依赖的Redis跟RDS也没有超水位告警。 真是见鬼，以往的排查方式居然没有一丝线索\u0026hellip; 于是我觉得自己加上耗时打点，然后通过脚本采集下耗时情况。  采集  接口加入打点统计不同机器上的mget耗时，redis未命中数， db并发查询耗时 在负载不高的机器分固定uid 跟随机uid两组请求接口获取耗时数据，同时本地直接连接redis调用mget统计耗时。  数据分析  计算每分钟各项数据耗时的平均值(这里走了个弯路😰) 对比不同机器上的mget耗时，看是不是个别机器的问题。 对比同时需要请求Redis跟DB时，了解mget跟db的耗时偏差。 对比服务mget跟本地mget耗时，验证Redis耗时情况是否如实。  以下图标x轴为时间（分钟级）y轴为耗时（单位ms) 上面这张就是不同机器的mget耗时情况，可以看出mget耗时确实存在高峰期效应并且不存在个别机器耗时偏离较大的情况，所以排除个别机器的问题。\n在同时需要查Redis跟DB的请求中，DB耗时明显高于Redis,但这是可预见的，高低峰两个曲线的波动从这个图上看不明显，进一步通过抽查服务的打点日志可以看到3万个请求中需要查询DB的只有16个，说明feed流获取的用户基本都是缓存到Redis的热数据，排除DB引发高耗时,进一步观察Redis的数据。\n上面这张图左边是出问题的服务内部mget，右边是低负载服务器上使用同个Redis集群的mget表现。两者呈现相同的分布趋势，Redis高峰期确实响应变慢。\n通过以上的一轮分析，首先排除了个别机器的问题以及DB的问题，似乎问题就是Redis慢了导致的。但是，不对啊，Redis耗时虽然高峰期变慢但是8ms也并不至于导致100+ms的告警，另外Redis的性能是大家公认的。 后面经过和同事的一番探讨，收获了一个重要的知识\u0026ndash;排查问题的时候一般看最大值，另外也还发现我漏分析了中转的耗时。\n上面这张就是后来补的中转耗时趋势图（按每分钟取最大值处理），中转最大耗时也不过2ms，显然也不是中转导致的。\n继续对之前的数据做最大值处理，在服务端mget跟本地mget这组队比中我们发现了异常👇\n在取最大值的情况下，服务的mget耗时高峰期普遍到达80ms以上，而直接连Redis的对照组则十分平稳没有高峰效应。这十分让人不解，我的脚本连接Redis跟服务器的表现完全对不上了，太奇怪了，是时候分析下代码了。\n查看代码  我的脚本跟服务器都是通过这样的方式获取Redis客户端，这个客户端到底是怎么生成的呢？option里到底有什么？   通过对代码的进一步查看，发现这个Redis客户端是维护了一组Redis链接池的，并且通过这个poolSize的参数设置链接池的大小，默认大小是10*CPU数。这下我们有了线索，初步怀疑连接不够用，继续看代码，发现连接池无空闲连接的时候默认的等待超时居然达1秒钟，这更加决定了我的怀疑，后续我加了个空闲连接数的带点也确实验证了高峰期连接数耗尽的情况。  解决  修改默认连接池数目，因为通过代码发现这个连接数是有自动回收机制的，连接空闲超过一定时间就会被回收。  总结  采用耗时打点的方式可以很快定位问题的位置。 设置合理的对照组有利于体现数据异常从中找到线索。 分析性能问题时要看最大值，平均值会掩盖掉真实情况。 最后要耐心地分析代码尝试到数据背后的真相。  ","date":"2020-07-31","permalink":"/posts/record-server-optimization/","tags":["redis"],"title":"记一次服务端性能优化"},{"content":"SSH是什么 ssh 是一种加密网络协议，通过建立安全隧道来进行客户端/服务器通信，通常用于远程登录，但实际上任何网络服务都可以通过ssh进行安全传输。\n利用SSH 进行科学上网  如果你恰巧有一台海外服务器，可以利用ssh进行动态端口转发从而实现科学上网。 ssh -p your_ssh_port -f -N -D 0.0.0.0:8000 username@remote_host 这个命令启动socket监听本地端口8000,将所有发送到的数据通过ssh协议与远程主机进行通信。 开启动态端口转发后，通过SwitchyOmega等代理插件将浏览器数据转发到本地端口8000即可。 定时重启脚本。不知道是不是超时配置问题，ssh会时不时断线，需要重启，所以就写了个python脚本加到crontab。  import subprocess import os import re ''' 通过 ps -ef | grep ssh\\ -p\\ 22\\ -f\\ -N | grep -v grep | awk '{print $2}' 找到启动的进程，执行 kill， 重启ssh转发命令。 ''' try: ps = subprocess.Popen(('ps', '-ef'), stdout=subprocess.PIPE) grep = subprocess.Popen(('grep', 'ssh\\ -p\\ 22\\ -f\\ -N' ), stdin=ps.stdout, stdout=subprocess.PIPE) awk = subprocess.check_output(('awk', '{print $2}'), stdin=grep.stdout) ps.wait() grep.wait() print(awk) except: command = \u0026quot;ssh -p 22 -f -N -D 0.0.0.0:8000 username@remote_ip\u0026quot; os.system(command) exit() pids = [p for p in awk.split('\\n') if re.match(\u0026quot;^\\d\u0026quot;, p)] for pid in pids: print(\u0026quot;to kill...%s\u0026quot; % (pid)) subprocess.check_output([\u0026quot;kill\u0026quot;, pid]) command = \u0026quot;ssh -p 22 -f -N -D 0.0.0.0:8000 username@remote_ip\u0026quot; os.system(command)   用shell 更少代码  #!/usr/bin pids=(`ps -ef | grep 'ssh -p 22 -f -N' | grep -v 'grep' | awk '{print $2}'`) echo $pids for p in $pids do kill -9 $p\tdone ssh -p 22 -f -N -D 0.0.0.0:1090 root@remote_ip  ","date":"2019-06-10","permalink":"/posts/access-google/","tags":["ssh"],"title":"科学上网之ssh"},{"content":"记录Java8的一些语法🍬\nList to Map @Data @AllArgsConstructor @ToString class People { private String name; private int age; private String lastName; } @Test public void testStream() { List\u0026lt;People\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(new People(\u0026quot;nautilis\u0026quot;, 25, \u0026quot;zheng\u0026quot;)); list.add(new People(\u0026quot;nautilis\u0026quot;, 25, \u0026quot;zheng\u0026quot;)); list.add(new People(\u0026quot;bevan\u0026quot;, 25, \u0026quot;zheng\u0026quot;)); list.add(new People(\u0026quot;ning\u0026quot;, 24, \u0026quot;li\u0026quot;)); //重复key只保留一个， 并用linkedHashMap保证顺序 Map\u0026lt;String, People\u0026gt; map = list.stream().sorted(Comparator.comparing(People::getAge).reversed()) .collect(Collectors.toMap(People::getName, Function.identity(), (oldv, newV) -\u0026gt; newV, LinkedHashMap::new )); System.out.printf(\u0026quot;重复key只保留一个， 并用linkedHashMap保证顺序---\u0026gt;\u0026quot;); map.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot;:\u0026quot; + v)); //姓氏#=\u0026gt;人列表 Map\u0026lt;String,List\u0026lt;People\u0026gt;\u0026gt; lastPeopleMap = list.stream() .collect(Collectors.groupingBy(People::getLastName)); System.out.println(\u0026quot;姓氏#=\u0026gt;人列表 ---\u0026gt;\u0026quot;); lastPeopleMap.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot; : \u0026quot; + v)); //姓氏#=\u0026gt; 人名列表 Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; lastnameNamelistMap = list.stream() .collect(Collectors.groupingBy(People::getLastName, Collectors.mapping(People::getName, Collectors.toList()))); System.out.println(\u0026quot;姓氏#=\u0026gt; 人名列表---\u0026gt;\u0026quot;); lastnameNamelistMap.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot; : \u0026quot; + v)); // 姓氏#=\u0026gt;人名集合 Map\u0026lt;String, Set\u0026lt;String\u0026gt;\u0026gt; lastnameNameSet = list.stream() .collect(Collectors.groupingBy(People::getLastName, Collectors.mapping(People::getName, Collectors.toSet()))); System.out.println(\u0026quot;姓氏#=\u0026gt;人名集合---\u0026gt;\u0026quot;); lastnameNameSet.forEach((k, v) -\u0026gt; System.out.println(k + \u0026quot; : \u0026quot; + v)); }  ","date":"2019-05-18","permalink":"/posts/java8/","tags":["Java"],"title":"Java8语法糖"},{"content":" 端口转发 http://www.dshowing.com/2017/08/14/SSH_portforward/ 本地启动ssh:ssh -p [serverport] -f -N -D 0.0.0.0:[socksport] username@remoteaddress chrome上用proxy switchysharp 或者 firefox 用FoxyProxy 配置一个socks5 IP:127.0.0.1 端口为所填的socksport。 ssh -o PubkeyAuthentication=no 120.24.161.131  不用ssh key ssh -f 后台运行 -N 不发送命令 -f -N 参数 ssh -f -N -L localport:host1:port1 host3 host2执行此命令， host2登录host3 本地端口转发 -L 来自host3走localhost的数据发送到host1的port1 远程端口转发 ssh -f -N -R remotePort:host1:port1 host3 在host2执行此命令表示 在host2登录host3 令 host3监听 remotePort, host2将host3发到 remotePort 的数据转发到host1:prot1 本地端口转发使用场景 host3可以连host2, host2能连host1,host3不能访问host1, 可通过host2做本地端口转发。 远程端口转发，用于内网穿透， host1 host2在内网，host2可访问外网的host3, host3无法访问内网，可以在host2做远程端口转发，使host3的数据发送到内网。 ssh -f -N -L localport:localhost:port host3 localhost指的是host3, localhost 是相对host3的。  ","date":"2019-03-21","permalink":"/posts/ssh-note/","tags":["ssh"],"title":"ssh 笔记"},{"content":"归并排序  归并排序是建立在归并操作的基础之上的。如果两个数组都是有序数组，只需迭代两个数组，不断比较两个数组元素，将较小的排到前面即可。归并排序首先通过递归对数组进行对等分割，直到分割的部分只有一个元素(相邻两个数组必定有序）进行合并操作。  #include\u0026lt;stdio.h\u0026gt; void show(int arr[], int n){ int i; for( i=0; i\u0026lt;n; i++) printf(\u0026quot;%d, \u0026quot;, arr[i]); printf(\u0026quot;\\n\u0026quot;); }\t//合并算法 void merge(int array[], int left, int mid, int right){ int aux[right+1]; //临时数组 //从中间断开两个标志分别 int i = left; int j = mid + 1; int k; for( k=left; k\u0026lt;=right; k++ ) //复制数组 aux[k] = array[k]; for( k=left; k\u0026lt;=right; k++ ){ if(i \u0026gt; mid) //左边指针已超出 array[k] = aux[j++]; else if(j \u0026gt; right) //右边指针超出 array[k] = aux[i++]; else if(aux[j] \u0026lt; aux[i]) //都没有超出就比较两者大小 array[k] = aux[j++]; else array[k] = aux[i++]; } } void sort(int array[], int left, int right){ if(left \u0026gt;= right) return;\tint mid = left + (right - left) / 2; sort(array, left, mid); sort(array, mid+1, right); merge(array, left, mid, right); } int main() { int arr_test[] = { 8, 2, 19, 4, 5, 11, 23, 44, 12, 31 }; int size = sizeof(arr_test)/sizeof(arr_test[0]); printf(\u0026quot;%s \\n\u0026quot;, \u0026quot;before sort:\u0026quot;); show(arr_test, size); sort(arr_test, 0, size-1);\tprintf(\u0026quot;%s \\n\u0026quot;, \u0026quot;after sorted:\u0026quot;); show(arr_test, size); return 0; }  快速排序  快速排序是建立在划分基础上的，划分过程选取一个数组成员作为比较将数组划分成大于这比较目标和小于这个比较目标的两部分,对数组不断的递归划分最终数组即是有序的。  #include\u0026lt;stdio.h\u0026gt; void show(int arr[], int size) { int i; for(i=0;i\u0026lt;size;i++) printf(\u0026quot;%d \u0026quot;, arr[i]); printf(\u0026quot;\\n\u0026quot;); } void exch(int arr[], int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } int partition(int arr[], int left, int right) { int v = arr[left]; int i = left; //向前扫的指针 int j = right + 1; //向后扫指针 while(1){ while(arr[++i] \u0026lt; v) if(i == right)//前扫指针已到低 break; while(arr[--j] \u0026gt; v) if(j == left) //后扫指针已到低 break; if(i\u0026gt;=j)// 两个指针相遇 break;\texch(arr, i, j); //while 循环停下来交换元素 } exch(arr, left, j); return j; } void sort(int arr[], int left, int right) { if(left \u0026gt;= right) return; int j = partition(arr, left, right); sort(arr, left, j); sort(arr, j+1, right); } void quickSort(int arr[], int size) { sort(arr, 0, size -1); } int main() { int arr[] = {23, 11, 24, 5, 67, 2, 4}; int size = sizeof(arr) / sizeof(arr[0]); show(arr, size); quickSort(arr, size); show(arr, size); }  ","date":"2018-07-07","permalink":"/posts/sort-algorithm/","tags":["sort"],"title":"排序算法"}]